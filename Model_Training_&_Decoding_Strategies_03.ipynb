{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrCm4ATX96X3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file=input(\"Enter the file name: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQ3odqosFDHU",
        "outputId": "9f5d85f0-e4ad-49b5-e0be-7251f881d4ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the file name: the-verdict.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open (file, 'r') as f:\n",
        "  raw_text=f.read()\n",
        "\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixMZT06lF1jG",
        "outputId": "1ccc102e-23ad-499c-8bcf-ad6e9e903dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer_v1(nn.Module):\n",
        "  def __init__(self,tokenizer):\n",
        "    super().__init__()\n",
        "    self.tokenizer=tokenizer\n",
        "\n",
        "\n",
        "  def encode(self,text):\n",
        "    encoded_ids=self.tokenizer.encode(text,allowed_special={'<|endoftext|>'})\n",
        "    encoded_tokens=torch.tensor(encoded_ids).unsqueeze(0)\n",
        "    return encoded_tokens\n",
        "\n",
        "  def decode(self,ids):\n",
        "    flat=ids.squeeze(0)\n",
        "    decoded_txt=self.tokenizer.decode(flat.tolist())\n",
        "    return decoded_txt\n"
      ],
      "metadata": {
        "id": "ztBlDRmbGHdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiJcj2wrHlHU",
        "outputId": "626105d3-55b8-4855-be5d-9eae2374e62f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer=tiktoken.get_encoding('gpt2')"
      ],
      "metadata": {
        "id": "I_occijgHiSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset_v1(Dataset):\n",
        "\n",
        "  def __init__(self,txt,tokenizer,max_length,stride):\n",
        "    super().__init__()\n",
        "    self.input=[]\n",
        "    self.target=[]\n",
        "\n",
        "\n",
        "    token_ids= tokenizer.encode(txt,allowed_special={'<|endoftext|>'})\n",
        "    for i in range(0,len(token_ids)-max_length,stride):\n",
        "      self.input_ids=token_ids[i:i+max_length]\n",
        "      self.target_ids=token_ids[i+1:i+max_length+1]\n",
        "\n",
        "      self.input.append(torch.tensor(self.input_ids))\n",
        "      self.target.append(torch.tensor(self.target_ids))\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return self.input[idx],self.target[idx]"
      ],
      "metadata": {
        "id": "Wzi9HIs8Hx1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader_v1:\n",
        "\n",
        "  def __init__(self,txt,max_length=256,stride=128,batch_size=4,\n",
        "               drop_last=True,shuffle=True,num_workers=0):\n",
        "\n",
        "    tokenizer=tiktoken.get_encoding('gpt2')\n",
        "    dataset=Dataset_v1(txt,tokenizer,max_length,stride)\n",
        "\n",
        "    dataLoader=DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        drop_last=drop_last,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataLoader\n"
      ],
      "metadata": {
        "id": "Y8Wx061ZO1cW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Multi_head_attention_v1(nn.Module):\n",
        "\n",
        "  def __init__(self,d_in,d_out,num_heads,drop_out,context_length,qvk_bias=False):\n",
        "    super().__init__()\n",
        "    assert(d_out%num_heads==0), \\\n",
        "          \"d_out must be divisible by num_heads\"\n",
        "\n",
        "    self.d_out=d_out\n",
        "    self.num_heads=num_heads\n",
        "    self.head_dim=d_out//num_heads\n",
        "    self.w_query=nn.Linear(d_in,d_out,qvk_bias=qvk_bias)\n",
        "    self.w_key=nn.Linear(d_in,d_out,qvk_bias=qvk_bias)\n",
        "    self.w_value=nn.Linear(d_in,d_out,qvk_bias=qvk_bias)\n",
        "    self.out_proj=nn.Linear(d_out,d_out)\n",
        "    self.dropout=nn.Dropout(drop_out)\n",
        "    self.register_buffer('mask', torch.triu(torch.ones(context_length,context_length), diagonal=1))\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    b,num_tokens,d_in=x.shape\n",
        "    query=self.w_query(x)\n",
        "    key=self.w_key(x)\n",
        "    value=self.w_value(x)\n",
        "\n",
        "    query=query.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    key=key.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    value=value.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "\n",
        "    query=query.transpose(1,2)\n",
        "    key=key.transpose(1,2)\n",
        "    value=value.transpose(1,2)\n",
        "\n",
        "    attn_scores=query @ key.transpose(2,3)\n",
        "    attn_scores.masked_fill_(self.mask.bool()[:num_tokens,:num_tokens] , -torch.inf)\n",
        "    attn_weights= torch.softmax(attn_scores/key.shape[-1]**0.5, dim=-1)\n",
        "    attn_weights=self.dropout(attn_weights)\n",
        "\n",
        "    context_vector= (attn_weights @ value).transpose(1,2)\n",
        "    context_vector=context_vector.contiguous().view(b,num_tokens,self.d_out)\n",
        "    context_vector= self.out_proj(context_vector)\n",
        "\n",
        "    return context_vector\n"
      ],
      "metadata": {
        "id": "-8e2A5W7WKKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    'context_length': 1024,\n",
        "    'vocab_size': 50257,\n",
        "    'emb_dim': 768,\n",
        "    'n_heads': 12, # attention heads\n",
        "    'n_layers': 12, # num of transformer layers\n",
        "    'drop_rate': 0.1,\n",
        "    'qvk_bias': False\n",
        "}"
      ],
      "metadata": {
        "id": "SCGgW6W6ln1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU_v1(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self,x):\n",
        "    return  0.5*x* ( 1+ torch.tanh([torch.sqrt(torch.tensor(2/torch.pi))\n",
        "                                    * x+ 0.044715 * x**3]))\n",
        "\n",
        "class Feed_forward_v1(nn.Module):\n",
        "\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.layers=nn.Sequential([\n",
        "        nn.Linear(cfg['emb_dim'],cfg['emb_dim']*4),\n",
        "        GELU_v1(),\n",
        "        nn.Linear(cfg['emb_dim']*4,cfg['emb_dim'])\n",
        "    ])\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.layers(x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-3G7kWZSWa6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer_Norm(nn.Module):\n",
        "\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps=1e-5\n",
        "    self.scale=nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift=nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean=x.mean(dim=-1,keep_dim=True)\n",
        "    var=x.var(dim=-1,keep_dim=True,unbiased=False)\n",
        "    norm_x=x-mean/torch.sqrt(var +self.eps)\n",
        "\n",
        "    return self.scale * norm_x +self.shift"
      ],
      "metadata": {
        "id": "u1mQt9cT_9Zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer_Block(nn.Module):\n",
        "\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.att=Multi_head_attention_v1(cfg['emb_dim'],cfg['emb_dim'],\n",
        "                                     cfg['num_heads'],cfg['drop_rate'],\n",
        "                                     cfg['context_length'],qvk_bias=cfg['qvk_bias'])\n",
        "    self.norm1_layer=Layer_Norm(cfg['emb_dim'])\n",
        "    self.norm2_layer=Layer_Norm(cfg['emb_dim'])\n",
        "    self.ff=Feed_forward_v1(cfg)\n",
        "    self.dropout=nn.Dropout(cfg['drop_rate'])\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    shortcut=x\n",
        "    x=self.norm1_layer(x)\n",
        "    x=self.att(x)\n",
        "    x=self.dropout(x)\n",
        "    x=x+shortcut\n",
        "\n",
        "    shortcut=x\n",
        "    x=self.norm2_layer(x)\n",
        "    x=self.ff(x)\n",
        "    x=x+shortcut\n",
        "\n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hr9R_quICTCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GPT_model_v1(nn.Module):\n",
        "\n",
        "\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.token_embedding=nn.Linear(cfg['vocab_size'],cfg['emb_dim'])\n",
        "    self.pos_embedding=nn.Linear(cfg['context_length'],cfg['emb_dim'])\n",
        "    self.trf_blocks=nn.Sequential(\n",
        "        *[Transformer_Block(cfg) for _ in range(cfg['n_layers'])]\n",
        "    )\n",
        "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "    self.final_norm=Layer_Norm(cfg['emb_dim'])\n",
        "    self.out_head=nn.Linear(cfg['emb_dim'],cfg['vocab_size'])\n",
        "\n",
        "  def forward(self,token_ids):\n",
        "    batch_size,seq_len=token_ids.shape\n",
        "    tok_emb=self.token_embedding(token_ids)\n",
        "    pos_emb=self.pos_embedding(torch.arange(seq_len , device=token_ids.device))\n",
        "    input_emb=tok_emb+pos_emb\n",
        "    input_emb=self.drop_emb(input_emb)\n",
        "\n",
        "    x=self.trf_blocks(input_emb)\n",
        "    x=self.final_norm(x)\n",
        "    logits=self.out_head(x)\n",
        "\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "jGAo9JzLliiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model, idx,context_size,max_new_tokens):\n",
        "\n",
        "  #idx : (batches, num_tokens)\n",
        "\n",
        "\n",
        "\n",
        "  for _ in range(max_new_tokens):\n",
        "\n",
        "    idx_cond= idx[:,-context_size:]\n",
        "    with  model.no_grad():\n",
        "      logits= model(idx)\n",
        "\n",
        "    logits=logits[:, -1,:]\n",
        "    probas= torch.softmax(logits, dim=-1)\n",
        "    idx_new=torch.argmax(probas,dim=-1, keepdim=True)\n",
        "\n",
        "    idx=torch.cat((idx , idx_new),dim=1)\n",
        "\n",
        "  return idx"
      ],
      "metadata": {
        "id": "FdQslvHtO2at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss_Scratch(probas,targets):\n",
        "\n",
        "  #targets: (batches, num_tokens)\n",
        "  #probas: (batches, num_tokens, vocab_size)\n",
        "  b,n_tokens,vocab_size=probas.shape\n",
        "  total_prob=[]\n",
        "\n",
        "  for i in range(b):\n",
        "    target_probs= probas[i, [torch.arange(n_tokens)], targets[i]]\n",
        "\n",
        "    total_prob=torch.cat((total_prob,target_probs))\n",
        "\n",
        "  log_probs=torch.log(total_prob)\n",
        "  mean_prob=torch.mean(log_probs)\n",
        "  neg_log_likelihood= mean_prob*-1\n",
        "\n",
        "\n",
        "  return neg_log_likelihood\n",
        "\n"
      ],
      "metadata": {
        "id": "XIKggzDGW_GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity(loss):\n",
        "  return torch.exp(loss)"
      ],
      "metadata": {
        "id": "KVERqVLpcyru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch,target_batch,model,device):\n",
        "  input_batch,target_batch=input_batch.to(device), target_batch.to(device)\n",
        "  logits=model(input_batch)\n",
        "  loss=torch.nn.functional.cross_entropy(logits.flatten(0,1), target_batch.flatten())\n",
        "  return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "  total_loss=0\n",
        "\n",
        "  if len(data_loader)==0:\n",
        "    return float('nan')\n",
        "  elif num_batches==None:\n",
        "    num_batches= len(data_loader)\n",
        "  else:\n",
        "    num_batches=min(num_batches, len(data_loader))\n",
        "\n",
        "  for i,(x_batch, y_batch) in enumerate(data_loader):\n",
        "    if i<num_batches:\n",
        "      loss=calc_loss_batch(x_batch,y_batch,model,device)\n",
        "      total_loss+=1\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  return total_loss/num_batches\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jt_6TUCzhPKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop for LLM:"
      ],
      "metadata": {
        "id": "7QQzg2vRJQ-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    train_loss=calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "    test_loss=calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  return train_loss, test_loss\n"
      ],
      "metadata": {
        "id": "Y0EXchIgJI-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_train(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "  train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "  tokens_seen, global_step = 0, -1\n",
        "\n",
        "  for i in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    for input_batch,target_batch in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss=calc_loss_batch(input_batch, target_batch, model, device)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      tokens_seen+=input_batch.numel()\n",
        "      global_step += 1\n",
        "\n",
        "      # Optional evaluation step\n",
        "      if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {i+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "      generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ],
      "metadata": {
        "id": "nO26r70qNre_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "  pass\n",
        "  # just encoding and decoding occurs here\n",
        "\n"
      ],
      "metadata": {
        "id": "TFoJMMw3QeLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DECODING STRATEGY 1: TEMPERATURE SCALING"
      ],
      "metadata": {
        "id": "vZ2ZTMqHRU4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_with_temperature(logits, temperature):\n",
        "  temp_scaled=logits/temperature\n",
        "  probas= torch.softmax(temp_scaled, dim=0)\n",
        "  return probas\n",
        "\n"
      ],
      "metadata": {
        "id": "rswQ7DhQRC4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DECODING STRATEGY 2: Top-k sampling"
      ],
      "metadata": {
        "id": "LQxp19gNTaWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "next_token_logits = torch.tensor(\n",
        "[4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
        ")\n",
        "topk=3\n",
        "top_logits,top_pos=torch.topk(next_token_logits, topk)\n",
        "print(top_logits)\n",
        "print(top_pos)\n",
        "\n",
        "cond_tokens= torch.where(\n",
        "    condition=next_token_logits<top_logits[-1],\n",
        "    input=torch.tensor(float(\"-inf\")),\n",
        "    other=next_token_logits)\n",
        "\n",
        "print(cond_tokens)\n",
        "\n",
        "topk_probas=torch.softmax(cond_tokens,dim=-1)\n",
        "print(topk_probas)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luVgXN9vTTvU",
        "outputId": "b6e94de9-b468-41a5-91d7-769c4b6b79d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([6.7500, 6.2800, 4.5100])\n",
            "tensor([3, 7, 0])\n",
            "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n",
            "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q19RCF5QVRN_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}